{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.0"},"colab":{"name":"Copy of init.ipynb","provenance":[{"file_id":"https://github.com/epfl-ada/ada-2021-project-skrw/blob/main/init.ipynb","timestamp":1636713909307}],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"APA5qdQqwL4I"},"source":["# Milestone 2 Topic Trending Analysis\n","Nowadays, the media is full of overwhelming information in different topics and different areas. Although large amount of data is helpful in big data mining, most of data is not meaningful to us. In order to extract meaningful information from a large dataset which contains millions of quotations, trending topic analysis is a Natural Language Processing (NLP) technique that allows us to automatically extract meaningful information from text by identifying recurrent themes or topics. Hot topic analysis can help providing many meaningful information to be used in many recommendataion system, for example, social media monitoring tools and marketing."],"id":"APA5qdQqwL4I"},{"cell_type":"code","metadata":{"id":"4a7107aa-131d-41e3-8134-53a4e81a4504"},"source":["%load_ext autoreload\n","%autoreload 2\n","\n","import warnings; warnings.simplefilter('ignore')\n","import os, codecs, string, random\n","import numpy as np\n","from numpy.random import seed as random_seed\n","from numpy.random import shuffle as random_shuffle\n","import matplotlib.pyplot as plt\n","from itertools import chain\n","%matplotlib inline  \n","\n","seed = 42\n","random.seed(seed)\n","np.random.seed(seed)\n","\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","#NLP libraries\n","import spacy, nltk, gensim, sklearn\n","import pyLDAvis.gensim_models\n","\n","from nltk.tokenize import sent_tokenize\n","from nltk.tokenize import word_tokenize\n","from nltk import pos_tag\n","from nltk.corpus import wordnet\n","from nltk.stem.wordnet import WordNetLemmatizer\n","from nltk.corpus import stopwords\n","from gensim.models import Phrases\n","from gensim import models\n","from gensim import corpora\n","from gensim.models import CoherenceModel\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('wordnet')\n","nltk.download('stopwords')"],"id":"4a7107aa-131d-41e3-8134-53a4e81a4504","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"imrBB0lCwo1I"},"source":["## Load the dataset\n","We use the quote-2019.json and load it using chunksize 1000000. In total, we get the 22 chunks and aggregate the chunks by the date. \n","After that, we combine these 22 chunks to get a single dataframe with shape 365*2. One column is the date(by day), the other are all the quotations from the single day."],"id":"imrBB0lCwo1I"},{"cell_type":"code","metadata":{"id":"c300ba3c-a04f-48bb-b1ee-b76a8b2154a3"},"source":["filename = '/media/shanci/DataDisk/ada_quotebank/quotes-2019.json.bz2' \n","\n","def process_chunk(chunk, idx):\n","    chunk['date'] = chunk.date.apply(lambda x: x.date())  \n","    transformed = chunk.groupby('date')['quotation'].apply(lambda x:x.str.cat(sep=' ')).reset_index()\n","        \n","    transformed.to_json('/media/shanci/DataDisk/ada_quotebank/chunk_{}.json'.format(idx))\n","\n","with pd.read_json(filename, lines=True, compression='bz2', chunksize=1000000) as df_reader:\n","    for idx, chunk in enumerate(df_reader):\n","        process_chunk(chunk, idx)"],"id":"c300ba3c-a04f-48bb-b1ee-b76a8b2154a3","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b2f23969-fa62-46d6-a5ec-70193e7d59cb"},"source":["fileroot = '/media/shanci/DataDisk/ada_quotebank/' \n","json_list = []\n","for i in range(22):\n","    json = pd.read_json(fileroot + \"chunk_{}.json\".format(i))\n","    json_list.append(json)\n","    "],"id":"b2f23969-fa62-46d6-a5ec-70193e7d59cb","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4fa55913-08cb-417c-9498-4f995b74c78f"},"source":["total_json = pd.concat(json_list)\n","data = total_json.groupby('date')['quotation'].apply(lambda x:x.str.cat(sep=' ')).reset_index()\n","data.head()"],"id":"4fa55913-08cb-417c-9498-4f995b74c78f","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t6WFtrUuyUVC"},"source":["## Get Sentences\n","First data processing step is to separate the quotation to single sentences. This can be done by sent_tokenize from nltk.tokenize."],"id":"t6WFtrUuyUVC"},{"cell_type":"code","metadata":{"id":"reHeBIQgyy4V"},"source":["data['sentences'] = data.quotation.progress_map(sent_tokenize)\n","data['sentences'].head(1).tolist()[0][:3] # check the first day of the first three sentences"],"id":"reHeBIQgyy4V","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fwkAOzolzMbg"},"source":["## Tokenization\n","Here we will build our tokenizer. Tokenization is a process to seprate the sentences into single words and punctuations. This can be done by word_tokenize from nltk.tokenize"],"id":"fwkAOzolzMbg"},{"cell_type":"code","metadata":{"id":"glV6J1RFz6RM"},"source":["data['tokens_sentences'] = data['sentences'].progress_map(lambda sentences: [word_tokenize(sentence) for sentence in sentences])\n","print(data['tokens_sentences'].head(1).tolist()[0][:3]) # check the first day of the first three sentences"],"id":"glV6J1RFz6RM","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Xe4KMI830RVX"},"source":["## Part of Speech Tagging and Lemmatization\n","Lemmatisation is the process of grouping together the inflected forms of a word so they can be analysed as a single item, identified by the word's lemma, or dictionary form. Unlike stemming, lemmatisation depends on correctly identifying the intended part of speech and meaning of a word in a sentence, as well as within the larger context surrounding that sentence, such as neighboring sentences or even an entire document.\n","\n","Generally, lemmatization is followed by Part of Speech Tagging (POS-Tag), which is the labeling of the words in a text according to their word types (noun, adjective, adverb, verb, etc.) POS tagging is a supervised learning solution that uses features like the previous word, next word, is first letter capitalized etc. NLTK has a function to get pos tags and it works after tokenization process. Detailed tags can be found here(https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html).\n","\n","NLTK provides WordNetLemmatizer class which is a thin wrapper around the wordnet corpus."],"id":"Xe4KMI830RVX"},{"cell_type":"code","metadata":{"id":"o-KFxU4H0Qkn"},"source":["data['POS_tokens'] = data['tokens_sentences'].progress_map(lambda tokens_sentences: [pos_tag(tokens) for tokens in tokens_sentences])\n","print(data['POS_tokens'].head(1).tolist()[0][:3])"],"id":"o-KFxU4H0Qkn","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iy8bePLg2eMo"},"source":["def get_wordnet_pos(treebank_tag):\n","\n","    if treebank_tag.startswith('J'):\n","        return wordnet.ADJ\n","    elif treebank_tag.startswith('V'):\n","        return wordnet.VERB\n","    elif treebank_tag.startswith('N'):\n","        return wordnet.NOUN\n","    elif treebank_tag.startswith('R'):\n","        return wordnet.ADV\n","    else:\n","        return ''\n","\n","lemmatizer = WordNetLemmatizer()\n","data['tokens_sentences_lemmatized'] = data['POS_tokens'].progress_map(\n","    lambda list_tokens_POS: [\n","        [\n","            lemmatizer.lemmatize(el[0], get_wordnet_pos(el[1])) \n","            if get_wordnet_pos(el[1]) != '' else el[0] for el in tokens_POS\n","        ] \n","        for tokens_POS in list_tokens_POS\n","    ]\n",")"],"id":"iy8bePLg2eMo","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nFC0_lvS272B"},"source":["## Remove Stopwords\n","Stop words are a set of commonly used words in a language. Examples of stop words in English are “a”, “the”, “is”, “are” and etc. Stop words are commonly used in Text Mining and Natural Language Processing (NLP) to eliminate words that are so commonly used that they carry very little useful information.\n","\n","We have adopted stopwords file from https://github.com/ahmedbesbes/How-to-mine-newsfeed-data-and-extract-interactive-insights-in-Python/blob/master/data/stopwords.txt. We also add the stopword from nltk.corpus. Here you can also add your stopword into the additional_stop_words list."],"id":"nFC0_lvS272B"},{"cell_type":"code","metadata":{"id":"yBudQa1J3MPS"},"source":["stop_words = []\n","\n","f = open('stopwords.txt', 'r')\n","for l in f.readlines():\n","    stop_words.append(l.replace('\\n', ''))\n","    \n","additional_stop_words = ['t', 'will']\n","stop_words =  stop_words + additional_stop_words + stopwords.words('english')\n","print(\"The total stopword is {}\".format(len(stop_words)))"],"id":"yBudQa1J3MPS","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vzSXwWkr4G8O"},"source":["# They remove non ascii characters and standardize the text (can't -> cannot, i'm -> i am). This will make the tokenization process more efficient.\n","def _removeNonAscii(s): \n","    return \"\".join(i for i in s if ord(i)<128)\n","\n","def clean_text(text):\n","    text = text.lower()\n","    text = re.sub(r\"what's\", \"what is \", text)\n","    text = text.replace('(ap)', '')\n","    text = re.sub(r\"\\'s\", \" is \", text)\n","    text = re.sub(r\"\\'ve\", \" have \", text)\n","    text = re.sub(r\"can't\", \"cannot \", text)\n","    text = re.sub(r\"n't\", \" not \", text)\n","    text = re.sub(r\"i'm\", \"i am \", text)\n","    text = re.sub(r\"\\'re\", \" are \", text)\n","    text = re.sub(r\"\\'d\", \" would \", text)\n","    text = re.sub(r\"\\'ll\", \" will \", text)\n","    text = re.sub(r'\\W+', ' ', text)\n","    text = re.sub(r'\\s+', ' ', text)\n","    text = re.sub(r\"\\\\\", \"\", text)\n","    text = re.sub(r\"\\'\", \"\", text)    \n","    text = re.sub(r\"\\\"\", \"\", text)\n","    text = re.sub('[^a-zA-Z ?!]+', '', text)\n","    text = _removeNonAscii(text)\n","    text = text.strip()\n","    return text"],"id":"vzSXwWkr4G8O","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fJQsIOvWI3jq"},"source":["data['tokens'] = data['tokens_sentences_lemmatized'].map(lambda sentences: list(chain.from_iterable(sentences)))\n","data['tokens'] = data['tokens'].map(lambda tokens: [token.lower() for token in tokens if token.isalpha() \n","                                                    and token.lower() not in my_stopwords and len(token)>1])"],"id":"fJQsIOvWI3jq","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IbBW16t3KCVm"},"source":["## Phrase detection\n","We need to automatically detect common phrases like multi-word expressions, word n-gram collocations from a stream of tokens."],"id":"IbBW16t3KCVm"},{"cell_type":"code","metadata":{"id":"83SUKuaOJkTT"},"source":["tokens = data['tokens'].tolist()\n","bigram_model = Phrases(tokens)\n","trigram_model = Phrases(bigram_model[tokens], min_count=2)\n","tokens = list(trigram_model[bigram_model[tokens]])"],"id":"83SUKuaOJkTT","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jv1tK-3UdKYK"},"source":["## Create the dictionary\n","In Gensim, the dictionary object is used to create a bag of words (BoW) corpus which further used as the input to topic modelling and other models as well.\n","\n","Corpus − It refers to a collection of documents as a bag of words (BoW).\n","\n","Here we define three functions:\n","1. LDA_model, this function returns the LDA model\n","2. compute_coherence, this function is used to compute coherence score to evaluate the LDA model\n","3. display_topics, function to display topics and corresponding keywords:\n","3. explore model. this is a tuning function, used to explore different topics of LDA models."],"id":"jv1tK-3UdKYK"},{"cell_type":"code","metadata":{"id":"pe1IuCTFLYr4"},"source":["dictionary_LDA = corpora.Dictionary(tokens)\n","dictionary_LDA.filter_extremes(no_below=2)\n","corpus = [dictionary_LDA.doc2bow(tok) for tok in tokens]\n","\n","def LDA_model(num_topics,corpus, id2word, passes=1):\n","    return gensim.models.ldamodel.LdaModel(corpus=corpus,\n","                                               id2word=dictionary_LDA,\n","                                               num_topics=num_topics, \n","                                               random_state=100,\n","                                               eval_every=10,\n","                                               chunksize=2000,\n","                                               passes=passes,\n","                                               per_word_topics=True\n","                                            )\n","def compute_coherence(model,tokens):\n","    coherence = CoherenceModel(model=model, \n","                           texts=tokens,\n","                           dictionary=dictionary_LDA, coherence='c_v')\n","    return coherence.get_coherence()\n","\n","def display_topics(model):\n","    topics = model.show_topics(num_topics=model.num_topics, formatted=False, num_words=10)\n","    topics = map(lambda c: map(lambda cc: cc[0], c[1]), topics)\n","    df = pd.DataFrame(topics)\n","    df.index = ['topic_{0}'.format(i) for i in range(model.num_topics)]\n","    df.columns = ['keyword_{0}'.format(i) for i in range(1, 10+1)]\n","    return df\n","\n","def explore_models(corpus,id2word,tokens, rg=range(5, 25)):\n","    models = []\n","    coherences = []\n","    \n","    for num_topics in rg:\n","        lda_model = LDA_model(num_topics,corpus, id2word, passes=5)\n","        models.append(lda_model)\n","        coherence = compute_coherence(lda_model,tokens)\n","        coherences.append(coherence)\n","      \n","\n","    fig = plt.figure(figsize=(15, 5))\n","    plt.title('Choosing the optimal number of topics')\n","    plt.xlabel('Number of topics')\n","    plt.ylabel('Coherence')\n","    plt.grid(True)\n","    plt.plot(rg, coherences)\n","    \n","    return coherences, models\n","\n","\n","coherences, models = explore_models(corpus,id2word,tokens, rg=range(5, 5, 25))"],"id":"pe1IuCTFLYr4","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E2uIui8Stgcf"},"source":["best_model = LDA_model(num_topics=40, passes=5)\n","\n","display_topics(model=best_model)"],"id":"E2uIui8Stgcf","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BuyOUYUYtq-x"},"source":["Now let's build a document/topic matrix. A cell i,j is the probabily of topic j in the document i."],"id":"BuyOUYUYtq-x"},{"cell_type":"code","metadata":{"id":"wWwTv49ptpfA"},"source":["def get_document_topic_matrix(corpus, num_topics=best_model.num_topics):\n","    matrix = []\n","    for row in tqdm_notebook(corpus):\n","        output = np.zeros(num_topics)\n","        doc_proba = best_model[row][0]\n","        for doc, proba in doc_proba:\n","            output[doc] = proba\n","        matrix.append(output)\n","    matrix = np.array(matrix)\n","    return matrix\n","\n","matrix = get_document_topic_matrix(corpus)"],"id":"wWwTv49ptpfA","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LhGA5edRt6cD"},"source":["LDA outputs a distribution of topic for each document. We'll assume that a document's topic is the one with the highest probability."],"id":"LhGA5edRt6cD"},{"cell_type":"code","metadata":{"id":"qOFE7Twnt47T"},"source":["doc_topic = best_model.get_document_topics(corpus)\n","lda_keys = []\n","for i, desc in enumerate(data['quotation']):\n","    lda_keys.append(np.argmax(matrix[i, :]))\n","\n","run = False\n","if run: \n","    tsne_model = TSNE(n_components=2, verbose=1, random_state=0, n_iter=500)\n","    tsne_lda = tsne_model.fit_transform(matrix)\n","    lda_df = pd.DataFrame(tsne_lda, columns=['x', 'y'])\n","    lda_df['topic'] = lda_keys\n","    lda_df['topic'] = lda_df['topic'].map(str)\n","    lda_df['description'] = data['description']\n","    lda_df['category'] = data['category']\n","    # lda_df.to_csv('./data/tsne_lda.csv', index=False, encoding='utf-8')\n","else:\n","    lda_df = pd.read_csv('./data/tsne_lda.csv')\n","    lda_df['topic'] = lda_df['topic'].map(str)\n"],"id":"qOFE7Twnt47T","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d5P3Qfe1ubJe"},"source":["## NMF: Non-negative Matrix Factorization\n"],"id":"d5P3Qfe1ubJe"},{"cell_type":"code","metadata":{"id":"a-5j1SdxuaRs"},"source":["from sklearn.decomposition import NMF\n","\n","vectorizer = TfidfVectorizer(min_df=5, analyzer='word', ngram_range=(1, 2), stop_words='english')\n","vz = vectorizer.fit_transform(list(data['tokens'].map(lambda tokens: ' '.join(tokens))))\n","\n","nmf = NMF(n_components=40, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd').fit(vz)\n","\n","feature_names = vectorizer.get_feature_names()\n","no_top_words = 10\n","\n","for topic_idx, topic in enumerate(nmf.components_[:10]):\n","    print(\"Topic %d:\"% (topic_idx))\n","    print(\" | \".join([feature_names[i]\n","                    for i in topic.argsort()[:-no_top_words - 1:-1]]))"],"id":"a-5j1SdxuaRs","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nf5WFGc4pbzS"},"source":["## Visualization of LDA model"],"id":"nf5WFGc4pbzS"},{"cell_type":"code","metadata":{"id":"HiwD9DvRnjM2"},"source":["vis = pyLDAvis.gensim_models.prepare(topic_model=lda_model, corpus=corpus, dictionary=dictionary_LDA)\n","pyLDAvis.enable_notebook()\n","pyLDAvis.display(vis)"],"id":"HiwD9DvRnjM2","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AjofRLt2pfJt"},"source":["## TFIDF\n","tf-idf stands for term frequencey-inverse document frequency. It's a numerical statistic intended to reflect how important a word is to a document or a corpus (i.e a collection of documents).\n","\n","To relate to this post, words correpond to tokens and documents correpond to descriptions. A corpus is therefore a collection of descriptions."],"id":"AjofRLt2pfJt"},{"cell_type":"code","metadata":{"id":"KziYswP8pnQW"},"source":["vectorizer = TfidfVectorizer(min_df=5, analyzer='word', ngram_range=(1, 2), stop_words='english')\n","vz = vectorizer.fit_transform(tokens)\n","print(vz.shape)"],"id":"KziYswP8pnQW","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fH1vSxSWp8BJ"},"source":["vz is a tfidf matrix.\n","\n","its number of rows is the total number of documents (descriptions)\n","\n","its number of columns is the total number of unique terms (tokens) across the documents (descriptions)"],"id":"fH1vSxSWp8BJ"},{"cell_type":"code","metadata":{"id":"Q9Z1id8vp82m"},"source":["tfidf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))\n","tfidf = pd.DataFrame(columns=['tfidf']).from_dict(dict(tfidf), orient='index')\n","tfidf.columns = ['tfidf']\n","\n","tfidf.tfidf.hist(bins=25, figsize=(15,7))"],"id":"Q9Z1id8vp82m","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OiU2EiiUqNSa"},"source":["## WordCloud to visialise words\n","We can use wordcloud to display important word and unimportant world."],"id":"OiU2EiiUqNSa"},{"cell_type":"code","metadata":{"id":"UdvRmT81qKKm"},"source":["from wordcloud import WordCloud\n","\n","def plot_word_cloud(terms):\n","    text = terms.index\n","    text = ' '.join(list(text))\n","    # lower max_font_size\n","    wordcloud = WordCloud(max_font_size=40).generate(text)\n","    plt.figure(figsize=(25, 25))\n","    plt.imshow(wordcloud, interpolation=\"bilinear\")\n","    plt.axis(\"off\")\n","    plt.show()\n","\n","plot_word_cloud(tfidf.sort_values(by=['tfidf'], ascending=True).head(40))"],"id":"UdvRmT81qKKm","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oO0OQgCNqfCr"},"source":["plot_word_cloud(tfidf.sort_values(by=['tfidf'], ascending=False).head(40))"],"id":"oO0OQgCNqfCr","execution_count":null,"outputs":[]}]}